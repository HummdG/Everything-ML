{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed843ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a42eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data from the dataset\n",
    "\n",
    "training_data = datasets.FashionMNIST (\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f59aa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n",
      "tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 7, 3, 4, 1, 2, 4, 8, 0, 2, 5, 7, 9,\n",
      "        1, 4, 6, 0, 9, 3, 8, 8, 3, 3, 8, 0, 7, 5, 7, 9, 6, 1, 3, 7, 6, 7, 2, 1,\n",
      "        2, 2, 4, 4, 5, 8, 2, 2, 8, 4, 8, 0, 7, 7, 8, 5])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# dataloader allows teh data to become iterable so we can do stuff on the dataset\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batch_size)\n",
    "\n",
    "# X is the tensor of images - dataset that contains a 4D array or a rank 4 tensor \n",
    "# image tensors are usually split into 4 dimensions: N = batch size or the number of images in each tensor\n",
    "# C = number of channel (1 for gray images and 3 for coloured images)\n",
    "# H = height of the image\n",
    "# W = width of the image\n",
    "\n",
    "\n",
    "# y contains the labels for the images\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    print(y)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e290347b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# essentially you can choose a device where you want to run the model\n",
    "# GPUs allow for parallelisation because of multi threading\n",
    "# Therefore machine learning tasks can be given to GPUs to do usually\n",
    "# However, if the GPU is not available we use CPU\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "\n",
    "# model inherited from the nn.Module\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten() # attribute flatten \n",
    "        self.linear_relu_stack = nn.Sequential( # attribute whcih is just a sequential stack of linear and relu layers\n",
    "            nn.Linear(28*28,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    # the logits are the raw outputs of the NN - they mean nothing essentially, \n",
    "    # but the magnitude and direction define a likelihood of sorts.\n",
    "    # the bigger the number and the more positive the more likely it is that class.\n",
    "    # for a classification task of 3 classes for example, for each image \n",
    "    # you will get logits like this [2.5, -1.6, 1]. So this means it is likely the first class\n",
    "    # \n",
    "    # not a probability - probability achieved by using a normalisation technique\n",
    "    # for classifcation we use softmax: exp(z^i)/exp(z^j) for all j from 1 to n\n",
    "    # it makes the negative values of the logits into positive but small values (because of how exponentiation works)\n",
    "    # and then the division by all other values in teh vector normalises beteween 0 and 1 to make it into probabiliies\n",
    "    # softmax values are used in training, but we only pass in the logits pytorch handles the rest.\n",
    "    # \n",
    "    #   \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x) # long ass explanation - see above\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4fa65dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so cross entropy is essentially a way of comparing two probability distributions\n",
    "# low entropy means something is predictable and a high entropy means something is random\n",
    "# if you compute the cross entropy between your true distribution (which is just a one-hot vector)\n",
    "# a one hot vector is a vector like this [0, 0, 1] where the correct label class is signified as a 1 and everything else is a 0\n",
    "# your predicted disribution will be softamx(logits) for an image\n",
    "# then you compute the cross entropy -> H(p,q) = - sum(p(i)log(q(i))) for all classes from 0 to C\n",
    "# log(0) - log(1) are negative numbers so what you need the negative at the front to make it positive.\n",
    "#\n",
    "# benefit: it penalises confidently wrong guesses because if cross entropy loss is high for that guess then the weights are readjusted to make the model learn.\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3) # loss function traversed using stochastic gradient descent - more detail elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f921988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_fn, optimizer):\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X) # prediction of the model\n",
    "        loss = loss_fn(pred, y) # loss function takes the prediciton and the true value \n",
    "\n",
    "        loss.backward() # backward propogation - computes teh gradienst w.r.t all weights\n",
    "        optimizer.step() # apply gradient update to weights\n",
    "        optimizer.zero_grad() # remove gradient after batch done otherwise will accumulate for next batch\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "\n",
    "            loss, current = loss.item(), (batch+1)*len(X) # loss.item() - raw tensor loss value\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d1d9369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() # done for evaluation\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y, in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item() # loss across all batches - we are evaluating so need to see loss on all dataset\n",
    "            \n",
    "            ### Let’s break this down:\n",
    "            # 1. `pred.argmax(1)`  \n",
    "            #    - Finds the predicted class (highest logit) for each example in the batch  \n",
    "            #    - Output shape: `[batch_size]`\n",
    "\n",
    "            # 2. `(pred.argmax(1) == y)`  \n",
    "            #    - Compares predictions to true labels  \n",
    "            #    - Returns a boolean tensor (e.g. `[True, False, True, ...]`)\n",
    "\n",
    "            # 3. `.type(torch.float)`  \n",
    "            #    - Converts `True → 1.0`, `False → 0.0`  \n",
    "            #    - So now you can **sum** correct predictions\n",
    "\n",
    "            # 4. `.sum().item()`  \n",
    "            #    - Counts the number of correct predictions in the batch  \n",
    "            #    - `.item()` converts the tensor to a Python number\n",
    "            \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item() # number of correct prediciton in the batch compared to the total count\n",
    "    test_loss /= num_batches # then gives average loss across all batches\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "183dde06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------\n",
      "loss: 2.299093 [   64/60000]\n",
      "loss: 2.297385 [ 6464/60000]\n",
      "loss: 2.291677 [12864/60000]\n",
      "loss: 2.296449 [19264/60000]\n",
      "loss: 2.289654 [25664/60000]\n",
      "loss: 2.284484 [32064/60000]\n",
      "loss: 2.289665 [38464/60000]\n",
      "loss: 2.279704 [44864/60000]\n",
      "loss: 2.279803 [51264/60000]\n",
      "loss: 2.281067 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 2.273043 \n",
      "\n",
      "Epoch 2\n",
      "--------------\n",
      "loss: 2.270748 [   64/60000]\n",
      "loss: 2.270345 [ 6464/60000]\n",
      "loss: 2.258685 [12864/60000]\n",
      "loss: 2.270109 [19264/60000]\n",
      "loss: 2.257015 [25664/60000]\n",
      "loss: 2.244817 [32064/60000]\n",
      "loss: 2.258393 [38464/60000]\n",
      "loss: 2.239217 [44864/60000]\n",
      "loss: 2.242123 [51264/60000]\n",
      "loss: 2.237689 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.1%, Avg loss: 2.227954 \n",
      "\n",
      "Epoch 3\n",
      "--------------\n",
      "loss: 2.228221 [   64/60000]\n",
      "loss: 2.226067 [ 6464/60000]\n",
      "loss: 2.203115 [12864/60000]\n",
      "loss: 2.221745 [19264/60000]\n",
      "loss: 2.197811 [25664/60000]\n",
      "loss: 2.169492 [32064/60000]\n",
      "loss: 2.195470 [38464/60000]\n",
      "loss: 2.156736 [44864/60000]\n",
      "loss: 2.160656 [51264/60000]\n",
      "loss: 2.144715 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.4%, Avg loss: 2.132256 \n",
      "\n",
      "Epoch 4\n",
      "--------------\n",
      "loss: 2.138284 [   64/60000]\n",
      "loss: 2.128682 [ 6464/60000]\n",
      "loss: 2.080377 [12864/60000]\n",
      "loss: 2.109879 [19264/60000]\n",
      "loss: 2.061102 [25664/60000]\n",
      "loss: 2.005577 [32064/60000]\n",
      "loss: 2.044808 [38464/60000]\n",
      "loss: 1.967431 [44864/60000]\n",
      "loss: 1.976362 [51264/60000]\n",
      "loss: 1.931017 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 42.5%, Avg loss: 1.917092 \n",
      "\n",
      "Epoch 5\n",
      "--------------\n",
      "loss: 1.941316 [   64/60000]\n",
      "loss: 1.907034 [ 6464/60000]\n",
      "loss: 1.813545 [12864/60000]\n",
      "loss: 1.862048 [19264/60000]\n",
      "loss: 1.767539 [25664/60000]\n",
      "loss: 1.712324 [32064/60000]\n",
      "loss: 1.748724 [38464/60000]\n",
      "loss: 1.650467 [44864/60000]\n",
      "loss: 1.681588 [51264/60000]\n",
      "loss: 1.601648 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.601834 \n",
      "\n",
      "Epoch 6\n",
      "--------------\n",
      "loss: 1.659350 [   64/60000]\n",
      "loss: 1.613789 [ 6464/60000]\n",
      "loss: 1.493040 [12864/60000]\n",
      "loss: 1.556599 [19264/60000]\n",
      "loss: 1.468807 [25664/60000]\n",
      "loss: 1.445068 [32064/60000]\n",
      "loss: 1.457888 [38464/60000]\n",
      "loss: 1.392897 [44864/60000]\n",
      "loss: 1.425199 [51264/60000]\n",
      "loss: 1.340212 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 1.356310 \n",
      "\n",
      "Epoch 7\n",
      "--------------\n",
      "loss: 1.428147 [   64/60000]\n",
      "loss: 1.395376 [ 6464/60000]\n",
      "loss: 1.260387 [12864/60000]\n",
      "loss: 1.343726 [19264/60000]\n",
      "loss: 1.256276 [25664/60000]\n",
      "loss: 1.260317 [32064/60000]\n",
      "loss: 1.267864 [38464/60000]\n",
      "loss: 1.222399 [44864/60000]\n",
      "loss: 1.257452 [51264/60000]\n",
      "loss: 1.180320 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 1.193006 \n",
      "\n",
      "Epoch 8\n",
      "--------------\n",
      "loss: 1.269389 [   64/60000]\n",
      "loss: 1.246098 [ 6464/60000]\n",
      "loss: 1.094333 [12864/60000]\n",
      "loss: 1.201016 [19264/60000]\n",
      "loss: 1.100183 [25664/60000]\n",
      "loss: 1.120613 [32064/60000]\n",
      "loss: 1.141403 [38464/60000]\n",
      "loss: 1.097475 [44864/60000]\n",
      "loss: 1.134965 [51264/60000]\n",
      "loss: 1.069983 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.074967 \n",
      "\n",
      "Epoch 9\n",
      "--------------\n",
      "loss: 1.148879 [   64/60000]\n",
      "loss: 1.140602 [ 6464/60000]\n",
      "loss: 0.966937 [12864/60000]\n",
      "loss: 1.096103 [19264/60000]\n",
      "loss: 0.982089 [25664/60000]\n",
      "loss: 1.008619 [32064/60000]\n",
      "loss: 1.050626 [38464/60000]\n",
      "loss: 1.002240 [44864/60000]\n",
      "loss: 1.035311 [51264/60000]\n",
      "loss: 0.990447 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.986441 \n",
      "\n",
      "Epoch 10\n",
      "--------------\n",
      "loss: 1.049211 [   64/60000]\n",
      "loss: 1.062518 [ 6464/60000]\n",
      "loss: 0.867675 [12864/60000]\n",
      "loss: 1.016472 [19264/60000]\n",
      "loss: 0.896756 [25664/60000]\n",
      "loss: 0.917377 [32064/60000]\n",
      "loss: 0.982679 [38464/60000]\n",
      "loss: 0.932046 [44864/60000]\n",
      "loss: 0.954063 [51264/60000]\n",
      "loss: 0.931241 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.919715 \n",
      "\n",
      "Epoch 11\n",
      "--------------\n",
      "loss: 0.967014 [   64/60000]\n",
      "loss: 1.002841 [ 6464/60000]\n",
      "loss: 0.790852 [12864/60000]\n",
      "loss: 0.954573 [19264/60000]\n",
      "loss: 0.837919 [25664/60000]\n",
      "loss: 0.844250 [32064/60000]\n",
      "loss: 0.930791 [38464/60000]\n",
      "loss: 0.881416 [44864/60000]\n",
      "loss: 0.890631 [51264/60000]\n",
      "loss: 0.885551 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.869614 \n",
      "\n",
      "Epoch 12\n",
      "--------------\n",
      "loss: 0.900670 [   64/60000]\n",
      "loss: 0.956001 [ 6464/60000]\n",
      "loss: 0.730744 [12864/60000]\n",
      "loss: 0.906321 [19264/60000]\n",
      "loss: 0.796638 [25664/60000]\n",
      "loss: 0.787208 [32064/60000]\n",
      "loss: 0.890476 [38464/60000]\n",
      "loss: 0.844876 [44864/60000]\n",
      "loss: 0.842799 [51264/60000]\n",
      "loss: 0.849263 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.831317 \n",
      "\n",
      "Epoch 13\n",
      "--------------\n",
      "loss: 0.847277 [   64/60000]\n",
      "loss: 0.917582 [ 6464/60000]\n",
      "loss: 0.682568 [12864/60000]\n",
      "loss: 0.868323 [19264/60000]\n",
      "loss: 0.765852 [25664/60000]\n",
      "loss: 0.742486 [32064/60000]\n",
      "loss: 0.858019 [38464/60000]\n",
      "loss: 0.816808 [44864/60000]\n",
      "loss: 0.806938 [51264/60000]\n",
      "loss: 0.819138 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.800980 \n",
      "\n",
      "Epoch 14\n",
      "--------------\n",
      "loss: 0.803237 [   64/60000]\n",
      "loss: 0.883739 [ 6464/60000]\n",
      "loss: 0.642973 [12864/60000]\n",
      "loss: 0.838051 [19264/60000]\n",
      "loss: 0.741364 [25664/60000]\n",
      "loss: 0.706993 [32064/60000]\n",
      "loss: 0.831152 [38464/60000]\n",
      "loss: 0.794184 [44864/60000]\n",
      "loss: 0.779536 [51264/60000]\n",
      "loss: 0.793602 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.775936 \n",
      "\n",
      "Epoch 15\n",
      "--------------\n",
      "loss: 0.765713 [   64/60000]\n",
      "loss: 0.852994 [ 6464/60000]\n",
      "loss: 0.609597 [12864/60000]\n",
      "loss: 0.813370 [19264/60000]\n",
      "loss: 0.720723 [25664/60000]\n",
      "loss: 0.678203 [32064/60000]\n",
      "loss: 0.807778 [38464/60000]\n",
      "loss: 0.774692 [44864/60000]\n",
      "loss: 0.758006 [51264/60000]\n",
      "loss: 0.771290 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.754485 \n",
      "\n",
      "Epoch 16\n",
      "--------------\n",
      "loss: 0.732581 [   64/60000]\n",
      "loss: 0.824753 [ 6464/60000]\n",
      "loss: 0.581321 [12864/60000]\n",
      "loss: 0.792561 [19264/60000]\n",
      "loss: 0.702979 [25664/60000]\n",
      "loss: 0.654364 [32064/60000]\n",
      "loss: 0.786417 [38464/60000]\n",
      "loss: 0.757285 [44864/60000]\n",
      "loss: 0.740609 [51264/60000]\n",
      "loss: 0.751549 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.735521 \n",
      "\n",
      "Epoch 17\n",
      "--------------\n",
      "loss: 0.702915 [   64/60000]\n",
      "loss: 0.798888 [ 6464/60000]\n",
      "loss: 0.557087 [12864/60000]\n",
      "loss: 0.774720 [19264/60000]\n",
      "loss: 0.687438 [25664/60000]\n",
      "loss: 0.634287 [32064/60000]\n",
      "loss: 0.766480 [38464/60000]\n",
      "loss: 0.741389 [44864/60000]\n",
      "loss: 0.725797 [51264/60000]\n",
      "loss: 0.734028 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.718382 \n",
      "\n",
      "Epoch 18\n",
      "--------------\n",
      "loss: 0.676208 [   64/60000]\n",
      "loss: 0.775558 [ 6464/60000]\n",
      "loss: 0.535832 [12864/60000]\n",
      "loss: 0.759173 [19264/60000]\n",
      "loss: 0.674154 [25664/60000]\n",
      "loss: 0.617170 [32064/60000]\n",
      "loss: 0.747767 [38464/60000]\n",
      "loss: 0.726781 [44864/60000]\n",
      "loss: 0.712798 [51264/60000]\n",
      "loss: 0.718471 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.702649 \n",
      "\n",
      "Epoch 19\n",
      "--------------\n",
      "loss: 0.652236 [   64/60000]\n",
      "loss: 0.754573 [ 6464/60000]\n",
      "loss: 0.516969 [12864/60000]\n",
      "loss: 0.745262 [19264/60000]\n",
      "loss: 0.662876 [25664/60000]\n",
      "loss: 0.602468 [32064/60000]\n",
      "loss: 0.729820 [38464/60000]\n",
      "loss: 0.713162 [44864/60000]\n",
      "loss: 0.701212 [51264/60000]\n",
      "loss: 0.704560 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.688120 \n",
      "\n",
      "Epoch 20\n",
      "--------------\n",
      "loss: 0.630682 [   64/60000]\n",
      "loss: 0.735636 [ 6464/60000]\n",
      "loss: 0.500283 [12864/60000]\n",
      "loss: 0.732541 [19264/60000]\n",
      "loss: 0.653572 [25664/60000]\n",
      "loss: 0.589753 [32064/60000]\n",
      "loss: 0.712692 [38464/60000]\n",
      "loss: 0.700729 [44864/60000]\n",
      "loss: 0.690923 [51264/60000]\n",
      "loss: 0.691940 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.674650 \n",
      "\n",
      "Epoch 21\n",
      "--------------\n",
      "loss: 0.611327 [   64/60000]\n",
      "loss: 0.718562 [ 6464/60000]\n",
      "loss: 0.485405 [12864/60000]\n",
      "loss: 0.720728 [19264/60000]\n",
      "loss: 0.645831 [25664/60000]\n",
      "loss: 0.578784 [32064/60000]\n",
      "loss: 0.696262 [38464/60000]\n",
      "loss: 0.689590 [44864/60000]\n",
      "loss: 0.681736 [51264/60000]\n",
      "loss: 0.680495 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.662217 \n",
      "\n",
      "Epoch 22\n",
      "--------------\n",
      "loss: 0.593887 [   64/60000]\n",
      "loss: 0.703187 [ 6464/60000]\n",
      "loss: 0.472122 [12864/60000]\n",
      "loss: 0.709699 [19264/60000]\n",
      "loss: 0.639464 [25664/60000]\n",
      "loss: 0.569246 [32064/60000]\n",
      "loss: 0.680685 [38464/60000]\n",
      "loss: 0.679821 [44864/60000]\n",
      "loss: 0.673521 [51264/60000]\n",
      "loss: 0.670031 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.650744 \n",
      "\n",
      "Epoch 23\n",
      "--------------\n",
      "loss: 0.578363 [   64/60000]\n",
      "loss: 0.689301 [ 6464/60000]\n",
      "loss: 0.460253 [12864/60000]\n",
      "loss: 0.699185 [19264/60000]\n",
      "loss: 0.634314 [25664/60000]\n",
      "loss: 0.560926 [32064/60000]\n",
      "loss: 0.665953 [38464/60000]\n",
      "loss: 0.671449 [44864/60000]\n",
      "loss: 0.666188 [51264/60000]\n",
      "loss: 0.660367 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.640184 \n",
      "\n",
      "Epoch 24\n",
      "--------------\n",
      "loss: 0.564402 [   64/60000]\n",
      "loss: 0.676667 [ 6464/60000]\n",
      "loss: 0.449515 [12864/60000]\n",
      "loss: 0.689215 [19264/60000]\n",
      "loss: 0.630011 [25664/60000]\n",
      "loss: 0.553554 [32064/60000]\n",
      "loss: 0.652103 [38464/60000]\n",
      "loss: 0.664332 [44864/60000]\n",
      "loss: 0.659680 [51264/60000]\n",
      "loss: 0.651299 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.630440 \n",
      "\n",
      "Epoch 25\n",
      "--------------\n",
      "loss: 0.551672 [   64/60000]\n",
      "loss: 0.665298 [ 6464/60000]\n",
      "loss: 0.439734 [12864/60000]\n",
      "loss: 0.679627 [19264/60000]\n",
      "loss: 0.626309 [25664/60000]\n",
      "loss: 0.547013 [32064/60000]\n",
      "loss: 0.639030 [38464/60000]\n",
      "loss: 0.658429 [44864/60000]\n",
      "loss: 0.654100 [51264/60000]\n",
      "loss: 0.642668 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.621428 \n",
      "\n",
      "Epoch 26\n",
      "--------------\n",
      "loss: 0.539969 [   64/60000]\n",
      "loss: 0.654791 [ 6464/60000]\n",
      "loss: 0.430833 [12864/60000]\n",
      "loss: 0.670358 [19264/60000]\n",
      "loss: 0.623025 [25664/60000]\n",
      "loss: 0.541199 [32064/60000]\n",
      "loss: 0.626613 [38464/60000]\n",
      "loss: 0.653646 [44864/60000]\n",
      "loss: 0.649198 [51264/60000]\n",
      "loss: 0.634325 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.613072 \n",
      "\n",
      "Epoch 27\n",
      "--------------\n",
      "loss: 0.528981 [   64/60000]\n",
      "loss: 0.645024 [ 6464/60000]\n",
      "loss: 0.422608 [12864/60000]\n",
      "loss: 0.661504 [19264/60000]\n",
      "loss: 0.620091 [25664/60000]\n",
      "loss: 0.535969 [32064/60000]\n",
      "loss: 0.614955 [38464/60000]\n",
      "loss: 0.650005 [44864/60000]\n",
      "loss: 0.644903 [51264/60000]\n",
      "loss: 0.626166 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.605271 \n",
      "\n",
      "Epoch 28\n",
      "--------------\n",
      "loss: 0.518479 [   64/60000]\n",
      "loss: 0.635864 [ 6464/60000]\n",
      "loss: 0.415093 [12864/60000]\n",
      "loss: 0.653182 [19264/60000]\n",
      "loss: 0.617269 [25664/60000]\n",
      "loss: 0.531024 [32064/60000]\n",
      "loss: 0.604043 [38464/60000]\n",
      "loss: 0.647278 [44864/60000]\n",
      "loss: 0.641032 [51264/60000]\n",
      "loss: 0.618079 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.598018 \n",
      "\n",
      "Epoch 29\n",
      "--------------\n",
      "loss: 0.508301 [   64/60000]\n",
      "loss: 0.627303 [ 6464/60000]\n",
      "loss: 0.407957 [12864/60000]\n",
      "loss: 0.645051 [19264/60000]\n",
      "loss: 0.614251 [25664/60000]\n",
      "loss: 0.526528 [32064/60000]\n",
      "loss: 0.593679 [38464/60000]\n",
      "loss: 0.645182 [44864/60000]\n",
      "loss: 0.637516 [51264/60000]\n",
      "loss: 0.610165 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.591234 \n",
      "\n",
      "Epoch 30\n",
      "--------------\n",
      "loss: 0.498520 [   64/60000]\n",
      "loss: 0.619202 [ 6464/60000]\n",
      "loss: 0.401309 [12864/60000]\n",
      "loss: 0.637172 [19264/60000]\n",
      "loss: 0.611148 [25664/60000]\n",
      "loss: 0.522427 [32064/60000]\n",
      "loss: 0.583786 [38464/60000]\n",
      "loss: 0.643632 [44864/60000]\n",
      "loss: 0.634207 [51264/60000]\n",
      "loss: 0.602354 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.584837 \n",
      "\n",
      "Epoch 31\n",
      "--------------\n",
      "loss: 0.489017 [   64/60000]\n",
      "loss: 0.611517 [ 6464/60000]\n",
      "loss: 0.395067 [12864/60000]\n",
      "loss: 0.629586 [19264/60000]\n",
      "loss: 0.607731 [25664/60000]\n",
      "loss: 0.518604 [32064/60000]\n",
      "loss: 0.574375 [38464/60000]\n",
      "loss: 0.642633 [44864/60000]\n",
      "loss: 0.631059 [51264/60000]\n",
      "loss: 0.594694 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.578779 \n",
      "\n",
      "Epoch 32\n",
      "--------------\n",
      "loss: 0.479738 [   64/60000]\n",
      "loss: 0.604302 [ 6464/60000]\n",
      "loss: 0.389135 [12864/60000]\n",
      "loss: 0.622206 [19264/60000]\n",
      "loss: 0.603950 [25664/60000]\n",
      "loss: 0.514847 [32064/60000]\n",
      "loss: 0.565306 [38464/60000]\n",
      "loss: 0.642032 [44864/60000]\n",
      "loss: 0.628156 [51264/60000]\n",
      "loss: 0.586958 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.573016 \n",
      "\n",
      "Epoch 33\n",
      "--------------\n",
      "loss: 0.470639 [   64/60000]\n",
      "loss: 0.597351 [ 6464/60000]\n",
      "loss: 0.383563 [12864/60000]\n",
      "loss: 0.615156 [19264/60000]\n",
      "loss: 0.600127 [25664/60000]\n",
      "loss: 0.511342 [32064/60000]\n",
      "loss: 0.556789 [38464/60000]\n",
      "loss: 0.641820 [44864/60000]\n",
      "loss: 0.625481 [51264/60000]\n",
      "loss: 0.579255 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.567538 \n",
      "\n",
      "Epoch 34\n",
      "--------------\n",
      "loss: 0.461736 [   64/60000]\n",
      "loss: 0.590639 [ 6464/60000]\n",
      "loss: 0.378259 [12864/60000]\n",
      "loss: 0.608238 [19264/60000]\n",
      "loss: 0.596023 [25664/60000]\n",
      "loss: 0.507925 [32064/60000]\n",
      "loss: 0.548733 [38464/60000]\n",
      "loss: 0.641832 [44864/60000]\n",
      "loss: 0.622868 [51264/60000]\n",
      "loss: 0.571801 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.562306 \n",
      "\n",
      "Epoch 35\n",
      "--------------\n",
      "loss: 0.453149 [   64/60000]\n",
      "loss: 0.584312 [ 6464/60000]\n",
      "loss: 0.373235 [12864/60000]\n",
      "loss: 0.601504 [19264/60000]\n",
      "loss: 0.591671 [25664/60000]\n",
      "loss: 0.504505 [32064/60000]\n",
      "loss: 0.541142 [38464/60000]\n",
      "loss: 0.642184 [44864/60000]\n",
      "loss: 0.620380 [51264/60000]\n",
      "loss: 0.564385 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.557296 \n",
      "\n",
      "Epoch 36\n",
      "--------------\n",
      "loss: 0.444655 [   64/60000]\n",
      "loss: 0.578199 [ 6464/60000]\n",
      "loss: 0.368533 [12864/60000]\n",
      "loss: 0.594960 [19264/60000]\n",
      "loss: 0.586991 [25664/60000]\n",
      "loss: 0.500945 [32064/60000]\n",
      "loss: 0.534012 [38464/60000]\n",
      "loss: 0.642764 [44864/60000]\n",
      "loss: 0.617935 [51264/60000]\n",
      "loss: 0.557052 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.552498 \n",
      "\n",
      "Epoch 37\n",
      "--------------\n",
      "loss: 0.436408 [   64/60000]\n",
      "loss: 0.572381 [ 6464/60000]\n",
      "loss: 0.364053 [12864/60000]\n",
      "loss: 0.588573 [19264/60000]\n",
      "loss: 0.582155 [25664/60000]\n",
      "loss: 0.497436 [32064/60000]\n",
      "loss: 0.527212 [38464/60000]\n",
      "loss: 0.643501 [44864/60000]\n",
      "loss: 0.615430 [51264/60000]\n",
      "loss: 0.549723 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.547885 \n",
      "\n",
      "Epoch 38\n",
      "--------------\n",
      "loss: 0.428336 [   64/60000]\n",
      "loss: 0.566826 [ 6464/60000]\n",
      "loss: 0.359771 [12864/60000]\n",
      "loss: 0.582373 [19264/60000]\n",
      "loss: 0.576812 [25664/60000]\n",
      "loss: 0.493868 [32064/60000]\n",
      "loss: 0.520708 [38464/60000]\n",
      "loss: 0.644313 [44864/60000]\n",
      "loss: 0.612902 [51264/60000]\n",
      "loss: 0.542568 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.543445 \n",
      "\n",
      "Epoch 39\n",
      "--------------\n",
      "loss: 0.420459 [   64/60000]\n",
      "loss: 0.561527 [ 6464/60000]\n",
      "loss: 0.355666 [12864/60000]\n",
      "loss: 0.576314 [19264/60000]\n",
      "loss: 0.571125 [25664/60000]\n",
      "loss: 0.490230 [32064/60000]\n",
      "loss: 0.514592 [38464/60000]\n",
      "loss: 0.645204 [44864/60000]\n",
      "loss: 0.610373 [51264/60000]\n",
      "loss: 0.535590 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.539172 \n",
      "\n",
      "Epoch 40\n",
      "--------------\n",
      "loss: 0.412800 [   64/60000]\n",
      "loss: 0.556397 [ 6464/60000]\n",
      "loss: 0.351647 [12864/60000]\n",
      "loss: 0.570441 [19264/60000]\n",
      "loss: 0.565300 [25664/60000]\n",
      "loss: 0.486595 [32064/60000]\n",
      "loss: 0.508868 [38464/60000]\n",
      "loss: 0.646157 [44864/60000]\n",
      "loss: 0.608053 [51264/60000]\n",
      "loss: 0.528767 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.535068 \n",
      "\n",
      "Epoch 41\n",
      "--------------\n",
      "loss: 0.405397 [   64/60000]\n",
      "loss: 0.551460 [ 6464/60000]\n",
      "loss: 0.347884 [12864/60000]\n",
      "loss: 0.564740 [19264/60000]\n",
      "loss: 0.559503 [25664/60000]\n",
      "loss: 0.482785 [32064/60000]\n",
      "loss: 0.503406 [38464/60000]\n",
      "loss: 0.647101 [44864/60000]\n",
      "loss: 0.605649 [51264/60000]\n",
      "loss: 0.522211 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.531129 \n",
      "\n",
      "Epoch 42\n",
      "--------------\n",
      "loss: 0.398135 [   64/60000]\n",
      "loss: 0.546695 [ 6464/60000]\n",
      "loss: 0.344308 [12864/60000]\n",
      "loss: 0.559245 [19264/60000]\n",
      "loss: 0.553873 [25664/60000]\n",
      "loss: 0.478919 [32064/60000]\n",
      "loss: 0.498337 [38464/60000]\n",
      "loss: 0.648319 [44864/60000]\n",
      "loss: 0.603410 [51264/60000]\n",
      "loss: 0.515819 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.527356 \n",
      "\n",
      "Epoch 43\n",
      "--------------\n",
      "loss: 0.391189 [   64/60000]\n",
      "loss: 0.542066 [ 6464/60000]\n",
      "loss: 0.340931 [12864/60000]\n",
      "loss: 0.553866 [19264/60000]\n",
      "loss: 0.548069 [25664/60000]\n",
      "loss: 0.475192 [32064/60000]\n",
      "loss: 0.493579 [38464/60000]\n",
      "loss: 0.649517 [44864/60000]\n",
      "loss: 0.601186 [51264/60000]\n",
      "loss: 0.509552 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.523750 \n",
      "\n",
      "Epoch 44\n",
      "--------------\n",
      "loss: 0.384466 [   64/60000]\n",
      "loss: 0.537650 [ 6464/60000]\n",
      "loss: 0.337756 [12864/60000]\n",
      "loss: 0.548609 [19264/60000]\n",
      "loss: 0.542205 [25664/60000]\n",
      "loss: 0.471381 [32064/60000]\n",
      "loss: 0.489160 [38464/60000]\n",
      "loss: 0.650705 [44864/60000]\n",
      "loss: 0.599056 [51264/60000]\n",
      "loss: 0.503497 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.520290 \n",
      "\n",
      "Epoch 45\n",
      "--------------\n",
      "loss: 0.377965 [   64/60000]\n",
      "loss: 0.533519 [ 6464/60000]\n",
      "loss: 0.334745 [12864/60000]\n",
      "loss: 0.543523 [19264/60000]\n",
      "loss: 0.536261 [25664/60000]\n",
      "loss: 0.467489 [32064/60000]\n",
      "loss: 0.484913 [38464/60000]\n",
      "loss: 0.651706 [44864/60000]\n",
      "loss: 0.596991 [51264/60000]\n",
      "loss: 0.497658 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.516964 \n",
      "\n",
      "Epoch 46\n",
      "--------------\n",
      "loss: 0.371707 [   64/60000]\n",
      "loss: 0.529573 [ 6464/60000]\n",
      "loss: 0.331846 [12864/60000]\n",
      "loss: 0.538721 [19264/60000]\n",
      "loss: 0.530330 [25664/60000]\n",
      "loss: 0.463676 [32064/60000]\n",
      "loss: 0.480881 [38464/60000]\n",
      "loss: 0.652599 [44864/60000]\n",
      "loss: 0.594998 [51264/60000]\n",
      "loss: 0.492017 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.513777 \n",
      "\n",
      "Epoch 47\n",
      "--------------\n",
      "loss: 0.365679 [   64/60000]\n",
      "loss: 0.525844 [ 6464/60000]\n",
      "loss: 0.329097 [12864/60000]\n",
      "loss: 0.534204 [19264/60000]\n",
      "loss: 0.524376 [25664/60000]\n",
      "loss: 0.459765 [32064/60000]\n",
      "loss: 0.477071 [38464/60000]\n",
      "loss: 0.653419 [44864/60000]\n",
      "loss: 0.592875 [51264/60000]\n",
      "loss: 0.486521 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.510730 \n",
      "\n",
      "Epoch 48\n",
      "--------------\n",
      "loss: 0.359891 [   64/60000]\n",
      "loss: 0.522272 [ 6464/60000]\n",
      "loss: 0.326461 [12864/60000]\n",
      "loss: 0.529824 [19264/60000]\n",
      "loss: 0.518529 [25664/60000]\n",
      "loss: 0.456044 [32064/60000]\n",
      "loss: 0.473313 [38464/60000]\n",
      "loss: 0.653978 [44864/60000]\n",
      "loss: 0.590686 [51264/60000]\n",
      "loss: 0.481295 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.507815 \n",
      "\n",
      "Epoch 49\n",
      "--------------\n",
      "loss: 0.354443 [   64/60000]\n",
      "loss: 0.518867 [ 6464/60000]\n",
      "loss: 0.323907 [12864/60000]\n",
      "loss: 0.525628 [19264/60000]\n",
      "loss: 0.512647 [25664/60000]\n",
      "loss: 0.452463 [32064/60000]\n",
      "loss: 0.469683 [38464/60000]\n",
      "loss: 0.654430 [44864/60000]\n",
      "loss: 0.588688 [51264/60000]\n",
      "loss: 0.476425 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.505027 \n",
      "\n",
      "Epoch 50\n",
      "--------------\n",
      "loss: 0.349245 [   64/60000]\n",
      "loss: 0.515559 [ 6464/60000]\n",
      "loss: 0.321457 [12864/60000]\n",
      "loss: 0.521587 [19264/60000]\n",
      "loss: 0.506847 [25664/60000]\n",
      "loss: 0.448947 [32064/60000]\n",
      "loss: 0.466309 [38464/60000]\n",
      "loss: 0.654751 [44864/60000]\n",
      "loss: 0.586572 [51264/60000]\n",
      "loss: 0.471783 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.502336 \n",
      "\n",
      "Epoch 51\n",
      "--------------\n",
      "loss: 0.344378 [   64/60000]\n",
      "loss: 0.512397 [ 6464/60000]\n",
      "loss: 0.319025 [12864/60000]\n",
      "loss: 0.517663 [19264/60000]\n",
      "loss: 0.501069 [25664/60000]\n",
      "loss: 0.445584 [32064/60000]\n",
      "loss: 0.463048 [38464/60000]\n",
      "loss: 0.654885 [44864/60000]\n",
      "loss: 0.584428 [51264/60000]\n",
      "loss: 0.467431 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.499760 \n",
      "\n",
      "Epoch 52\n",
      "--------------\n",
      "loss: 0.339678 [   64/60000]\n",
      "loss: 0.509342 [ 6464/60000]\n",
      "loss: 0.316732 [12864/60000]\n",
      "loss: 0.514047 [19264/60000]\n",
      "loss: 0.495376 [25664/60000]\n",
      "loss: 0.442489 [32064/60000]\n",
      "loss: 0.460001 [38464/60000]\n",
      "loss: 0.654776 [44864/60000]\n",
      "loss: 0.582391 [51264/60000]\n",
      "loss: 0.463338 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.497279 \n",
      "\n",
      "Epoch 53\n",
      "--------------\n",
      "loss: 0.335055 [   64/60000]\n",
      "loss: 0.506226 [ 6464/60000]\n",
      "loss: 0.314510 [12864/60000]\n",
      "loss: 0.510604 [19264/60000]\n",
      "loss: 0.489876 [25664/60000]\n",
      "loss: 0.439391 [32064/60000]\n",
      "loss: 0.457080 [38464/60000]\n",
      "loss: 0.654574 [44864/60000]\n",
      "loss: 0.580377 [51264/60000]\n",
      "loss: 0.459258 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.494900 \n",
      "\n",
      "Epoch 54\n",
      "--------------\n",
      "loss: 0.330580 [   64/60000]\n",
      "loss: 0.503173 [ 6464/60000]\n",
      "loss: 0.312297 [12864/60000]\n",
      "loss: 0.507260 [19264/60000]\n",
      "loss: 0.484432 [25664/60000]\n",
      "loss: 0.436534 [32064/60000]\n",
      "loss: 0.454283 [38464/60000]\n",
      "loss: 0.654271 [44864/60000]\n",
      "loss: 0.578363 [51264/60000]\n",
      "loss: 0.455242 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.492618 \n",
      "\n",
      "Epoch 55\n",
      "--------------\n",
      "loss: 0.326229 [   64/60000]\n",
      "loss: 0.500253 [ 6464/60000]\n",
      "loss: 0.310143 [12864/60000]\n",
      "loss: 0.504023 [19264/60000]\n",
      "loss: 0.479142 [25664/60000]\n",
      "loss: 0.433744 [32064/60000]\n",
      "loss: 0.451580 [38464/60000]\n",
      "loss: 0.653956 [44864/60000]\n",
      "loss: 0.576210 [51264/60000]\n",
      "loss: 0.451437 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.490415 \n",
      "\n",
      "Epoch 56\n",
      "--------------\n",
      "loss: 0.322056 [   64/60000]\n",
      "loss: 0.497513 [ 6464/60000]\n",
      "loss: 0.307928 [12864/60000]\n",
      "loss: 0.500982 [19264/60000]\n",
      "loss: 0.473837 [25664/60000]\n",
      "loss: 0.431101 [32064/60000]\n",
      "loss: 0.448873 [38464/60000]\n",
      "loss: 0.653562 [44864/60000]\n",
      "loss: 0.574064 [51264/60000]\n",
      "loss: 0.447895 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.488291 \n",
      "\n",
      "Epoch 57\n",
      "--------------\n",
      "loss: 0.317984 [   64/60000]\n",
      "loss: 0.494905 [ 6464/60000]\n",
      "loss: 0.305832 [12864/60000]\n",
      "loss: 0.498138 [19264/60000]\n",
      "loss: 0.468604 [25664/60000]\n",
      "loss: 0.428613 [32064/60000]\n",
      "loss: 0.446235 [38464/60000]\n",
      "loss: 0.652933 [44864/60000]\n",
      "loss: 0.571873 [51264/60000]\n",
      "loss: 0.444530 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.486247 \n",
      "\n",
      "Epoch 58\n",
      "--------------\n",
      "loss: 0.314115 [   64/60000]\n",
      "loss: 0.492325 [ 6464/60000]\n",
      "loss: 0.303805 [12864/60000]\n",
      "loss: 0.495513 [19264/60000]\n",
      "loss: 0.463606 [25664/60000]\n",
      "loss: 0.426275 [32064/60000]\n",
      "loss: 0.443698 [38464/60000]\n",
      "loss: 0.652193 [44864/60000]\n",
      "loss: 0.569739 [51264/60000]\n",
      "loss: 0.441286 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.484281 \n",
      "\n",
      "Epoch 59\n",
      "--------------\n",
      "loss: 0.310311 [   64/60000]\n",
      "loss: 0.489820 [ 6464/60000]\n",
      "loss: 0.301842 [12864/60000]\n",
      "loss: 0.492923 [19264/60000]\n",
      "loss: 0.458761 [25664/60000]\n",
      "loss: 0.424021 [32064/60000]\n",
      "loss: 0.441193 [38464/60000]\n",
      "loss: 0.651133 [44864/60000]\n",
      "loss: 0.567629 [51264/60000]\n",
      "loss: 0.438180 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.482370 \n",
      "\n",
      "Epoch 60\n",
      "--------------\n",
      "loss: 0.306644 [   64/60000]\n",
      "loss: 0.487368 [ 6464/60000]\n",
      "loss: 0.299989 [12864/60000]\n",
      "loss: 0.490460 [19264/60000]\n",
      "loss: 0.453961 [25664/60000]\n",
      "loss: 0.421992 [32064/60000]\n",
      "loss: 0.438771 [38464/60000]\n",
      "loss: 0.649839 [44864/60000]\n",
      "loss: 0.565595 [51264/60000]\n",
      "loss: 0.435284 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.480519 \n",
      "\n",
      "Epoch 61\n",
      "--------------\n",
      "loss: 0.303079 [   64/60000]\n",
      "loss: 0.484967 [ 6464/60000]\n",
      "loss: 0.298259 [12864/60000]\n",
      "loss: 0.488177 [19264/60000]\n",
      "loss: 0.449226 [25664/60000]\n",
      "loss: 0.420051 [32064/60000]\n",
      "loss: 0.436387 [38464/60000]\n",
      "loss: 0.648436 [44864/60000]\n",
      "loss: 0.563637 [51264/60000]\n",
      "loss: 0.432385 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.478722 \n",
      "\n",
      "Epoch 62\n",
      "--------------\n",
      "loss: 0.299616 [   64/60000]\n",
      "loss: 0.482548 [ 6464/60000]\n",
      "loss: 0.296646 [12864/60000]\n",
      "loss: 0.485986 [19264/60000]\n",
      "loss: 0.444643 [25664/60000]\n",
      "loss: 0.418239 [32064/60000]\n",
      "loss: 0.434060 [38464/60000]\n",
      "loss: 0.647071 [44864/60000]\n",
      "loss: 0.561654 [51264/60000]\n",
      "loss: 0.429646 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.476971 \n",
      "\n",
      "Epoch 63\n",
      "--------------\n",
      "loss: 0.296254 [   64/60000]\n",
      "loss: 0.480217 [ 6464/60000]\n",
      "loss: 0.295037 [12864/60000]\n",
      "loss: 0.483952 [19264/60000]\n",
      "loss: 0.440189 [25664/60000]\n",
      "loss: 0.416446 [32064/60000]\n",
      "loss: 0.431844 [38464/60000]\n",
      "loss: 0.645693 [44864/60000]\n",
      "loss: 0.559667 [51264/60000]\n",
      "loss: 0.427017 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.475267 \n",
      "\n",
      "Epoch 64\n",
      "--------------\n",
      "loss: 0.292977 [   64/60000]\n",
      "loss: 0.477970 [ 6464/60000]\n",
      "loss: 0.293430 [12864/60000]\n",
      "loss: 0.481971 [19264/60000]\n",
      "loss: 0.435775 [25664/60000]\n",
      "loss: 0.414793 [32064/60000]\n",
      "loss: 0.429751 [38464/60000]\n",
      "loss: 0.644195 [44864/60000]\n",
      "loss: 0.557647 [51264/60000]\n",
      "loss: 0.424496 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.473620 \n",
      "\n",
      "Epoch 65\n",
      "--------------\n",
      "loss: 0.289805 [   64/60000]\n",
      "loss: 0.475796 [ 6464/60000]\n",
      "loss: 0.291855 [12864/60000]\n",
      "loss: 0.480124 [19264/60000]\n",
      "loss: 0.431605 [25664/60000]\n",
      "loss: 0.413263 [32064/60000]\n",
      "loss: 0.427620 [38464/60000]\n",
      "loss: 0.642677 [44864/60000]\n",
      "loss: 0.555698 [51264/60000]\n",
      "loss: 0.422121 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.472022 \n",
      "\n",
      "Epoch 66\n",
      "--------------\n",
      "loss: 0.286697 [   64/60000]\n",
      "loss: 0.473665 [ 6464/60000]\n",
      "loss: 0.290271 [12864/60000]\n",
      "loss: 0.478377 [19264/60000]\n",
      "loss: 0.427522 [25664/60000]\n",
      "loss: 0.411730 [32064/60000]\n",
      "loss: 0.425627 [38464/60000]\n",
      "loss: 0.641177 [44864/60000]\n",
      "loss: 0.553706 [51264/60000]\n",
      "loss: 0.419767 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.470471 \n",
      "\n",
      "Epoch 67\n",
      "--------------\n",
      "loss: 0.283673 [   64/60000]\n",
      "loss: 0.471559 [ 6464/60000]\n",
      "loss: 0.288736 [12864/60000]\n",
      "loss: 0.476728 [19264/60000]\n",
      "loss: 0.423434 [25664/60000]\n",
      "loss: 0.410280 [32064/60000]\n",
      "loss: 0.423680 [38464/60000]\n",
      "loss: 0.639468 [44864/60000]\n",
      "loss: 0.551753 [51264/60000]\n",
      "loss: 0.417525 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.468947 \n",
      "\n",
      "Epoch 68\n",
      "--------------\n",
      "loss: 0.280796 [   64/60000]\n",
      "loss: 0.469524 [ 6464/60000]\n",
      "loss: 0.287251 [12864/60000]\n",
      "loss: 0.475192 [19264/60000]\n",
      "loss: 0.419399 [25664/60000]\n",
      "loss: 0.409005 [32064/60000]\n",
      "loss: 0.421784 [38464/60000]\n",
      "loss: 0.637676 [44864/60000]\n",
      "loss: 0.549825 [51264/60000]\n",
      "loss: 0.415356 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.467457 \n",
      "\n",
      "Epoch 69\n",
      "--------------\n",
      "loss: 0.277995 [   64/60000]\n",
      "loss: 0.467591 [ 6464/60000]\n",
      "loss: 0.285846 [12864/60000]\n",
      "loss: 0.473683 [19264/60000]\n",
      "loss: 0.415463 [25664/60000]\n",
      "loss: 0.407801 [32064/60000]\n",
      "loss: 0.419991 [38464/60000]\n",
      "loss: 0.635814 [44864/60000]\n",
      "loss: 0.547919 [51264/60000]\n",
      "loss: 0.413275 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.466000 \n",
      "\n",
      "Epoch 70\n",
      "--------------\n",
      "loss: 0.275267 [   64/60000]\n",
      "loss: 0.465664 [ 6464/60000]\n",
      "loss: 0.284470 [12864/60000]\n",
      "loss: 0.472222 [19264/60000]\n",
      "loss: 0.411682 [25664/60000]\n",
      "loss: 0.406679 [32064/60000]\n",
      "loss: 0.418181 [38464/60000]\n",
      "loss: 0.633952 [44864/60000]\n",
      "loss: 0.545982 [51264/60000]\n",
      "loss: 0.411305 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.464561 \n",
      "\n",
      "Epoch 71\n",
      "--------------\n",
      "loss: 0.272669 [   64/60000]\n",
      "loss: 0.463737 [ 6464/60000]\n",
      "loss: 0.283105 [12864/60000]\n",
      "loss: 0.470701 [19264/60000]\n",
      "loss: 0.408053 [25664/60000]\n",
      "loss: 0.405624 [32064/60000]\n",
      "loss: 0.416461 [38464/60000]\n",
      "loss: 0.632113 [44864/60000]\n",
      "loss: 0.544034 [51264/60000]\n",
      "loss: 0.409429 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.463154 \n",
      "\n",
      "Epoch 72\n",
      "--------------\n",
      "loss: 0.270171 [   64/60000]\n",
      "loss: 0.461806 [ 6464/60000]\n",
      "loss: 0.281740 [12864/60000]\n",
      "loss: 0.469325 [19264/60000]\n",
      "loss: 0.404454 [25664/60000]\n",
      "loss: 0.404688 [32064/60000]\n",
      "loss: 0.414852 [38464/60000]\n",
      "loss: 0.630159 [44864/60000]\n",
      "loss: 0.541987 [51264/60000]\n",
      "loss: 0.407589 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.461791 \n",
      "\n",
      "Epoch 73\n",
      "--------------\n",
      "loss: 0.267737 [   64/60000]\n",
      "loss: 0.459909 [ 6464/60000]\n",
      "loss: 0.280344 [12864/60000]\n",
      "loss: 0.467971 [19264/60000]\n",
      "loss: 0.400787 [25664/60000]\n",
      "loss: 0.403855 [32064/60000]\n",
      "loss: 0.413144 [38464/60000]\n",
      "loss: 0.628231 [44864/60000]\n",
      "loss: 0.540021 [51264/60000]\n",
      "loss: 0.405775 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.460437 \n",
      "\n",
      "Epoch 74\n",
      "--------------\n",
      "loss: 0.265418 [   64/60000]\n",
      "loss: 0.458110 [ 6464/60000]\n",
      "loss: 0.279043 [12864/60000]\n",
      "loss: 0.466676 [19264/60000]\n",
      "loss: 0.397294 [25664/60000]\n",
      "loss: 0.403021 [32064/60000]\n",
      "loss: 0.411480 [38464/60000]\n",
      "loss: 0.626316 [44864/60000]\n",
      "loss: 0.538127 [51264/60000]\n",
      "loss: 0.404085 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.459108 \n",
      "\n",
      "Epoch 75\n",
      "--------------\n",
      "loss: 0.263118 [   64/60000]\n",
      "loss: 0.456262 [ 6464/60000]\n",
      "loss: 0.277774 [12864/60000]\n",
      "loss: 0.465421 [19264/60000]\n",
      "loss: 0.394022 [25664/60000]\n",
      "loss: 0.402187 [32064/60000]\n",
      "loss: 0.409862 [38464/60000]\n",
      "loss: 0.624309 [44864/60000]\n",
      "loss: 0.536196 [51264/60000]\n",
      "loss: 0.402444 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.457800 \n",
      "\n",
      "Epoch 76\n",
      "--------------\n",
      "loss: 0.260860 [   64/60000]\n",
      "loss: 0.454527 [ 6464/60000]\n",
      "loss: 0.276582 [12864/60000]\n",
      "loss: 0.464103 [19264/60000]\n",
      "loss: 0.390877 [25664/60000]\n",
      "loss: 0.401464 [32064/60000]\n",
      "loss: 0.408401 [38464/60000]\n",
      "loss: 0.622356 [44864/60000]\n",
      "loss: 0.534320 [51264/60000]\n",
      "loss: 0.400851 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.456524 \n",
      "\n",
      "Epoch 77\n",
      "--------------\n",
      "loss: 0.258688 [   64/60000]\n",
      "loss: 0.452740 [ 6464/60000]\n",
      "loss: 0.275309 [12864/60000]\n",
      "loss: 0.462890 [19264/60000]\n",
      "loss: 0.387801 [25664/60000]\n",
      "loss: 0.400767 [32064/60000]\n",
      "loss: 0.406966 [38464/60000]\n",
      "loss: 0.620442 [44864/60000]\n",
      "loss: 0.532419 [51264/60000]\n",
      "loss: 0.399297 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.455273 \n",
      "\n",
      "Epoch 78\n",
      "--------------\n",
      "loss: 0.256617 [   64/60000]\n",
      "loss: 0.450957 [ 6464/60000]\n",
      "loss: 0.274133 [12864/60000]\n",
      "loss: 0.461672 [19264/60000]\n",
      "loss: 0.384894 [25664/60000]\n",
      "loss: 0.400128 [32064/60000]\n",
      "loss: 0.405494 [38464/60000]\n",
      "loss: 0.618454 [44864/60000]\n",
      "loss: 0.530544 [51264/60000]\n",
      "loss: 0.397735 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.454025 \n",
      "\n",
      "Epoch 79\n",
      "--------------\n",
      "loss: 0.254589 [   64/60000]\n",
      "loss: 0.449136 [ 6464/60000]\n",
      "loss: 0.272915 [12864/60000]\n",
      "loss: 0.460441 [19264/60000]\n",
      "loss: 0.381993 [25664/60000]\n",
      "loss: 0.399540 [32064/60000]\n",
      "loss: 0.404193 [38464/60000]\n",
      "loss: 0.616595 [44864/60000]\n",
      "loss: 0.528771 [51264/60000]\n",
      "loss: 0.396224 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.452801 \n",
      "\n",
      "Epoch 80\n",
      "--------------\n",
      "loss: 0.252613 [   64/60000]\n",
      "loss: 0.447344 [ 6464/60000]\n",
      "loss: 0.271734 [12864/60000]\n",
      "loss: 0.459244 [19264/60000]\n",
      "loss: 0.379080 [25664/60000]\n",
      "loss: 0.398850 [32064/60000]\n",
      "loss: 0.402861 [38464/60000]\n",
      "loss: 0.614768 [44864/60000]\n",
      "loss: 0.527001 [51264/60000]\n",
      "loss: 0.394813 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.451606 \n",
      "\n",
      "Epoch 81\n",
      "--------------\n",
      "loss: 0.250610 [   64/60000]\n",
      "loss: 0.445508 [ 6464/60000]\n",
      "loss: 0.270694 [12864/60000]\n",
      "loss: 0.458103 [19264/60000]\n",
      "loss: 0.376119 [25664/60000]\n",
      "loss: 0.398301 [32064/60000]\n",
      "loss: 0.401535 [38464/60000]\n",
      "loss: 0.612928 [44864/60000]\n",
      "loss: 0.525317 [51264/60000]\n",
      "loss: 0.393516 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.450421 \n",
      "\n",
      "Epoch 82\n",
      "--------------\n",
      "loss: 0.248726 [   64/60000]\n",
      "loss: 0.443650 [ 6464/60000]\n",
      "loss: 0.269598 [12864/60000]\n",
      "loss: 0.457116 [19264/60000]\n",
      "loss: 0.373176 [25664/60000]\n",
      "loss: 0.397657 [32064/60000]\n",
      "loss: 0.400221 [38464/60000]\n",
      "loss: 0.611132 [44864/60000]\n",
      "loss: 0.523682 [51264/60000]\n",
      "loss: 0.392186 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.449259 \n",
      "\n",
      "Epoch 83\n",
      "--------------\n",
      "loss: 0.246876 [   64/60000]\n",
      "loss: 0.441883 [ 6464/60000]\n",
      "loss: 0.268617 [12864/60000]\n",
      "loss: 0.456094 [19264/60000]\n",
      "loss: 0.370321 [25664/60000]\n",
      "loss: 0.397056 [32064/60000]\n",
      "loss: 0.399016 [38464/60000]\n",
      "loss: 0.609288 [44864/60000]\n",
      "loss: 0.522048 [51264/60000]\n",
      "loss: 0.390931 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.448107 \n",
      "\n",
      "Epoch 84\n",
      "--------------\n",
      "loss: 0.245157 [   64/60000]\n",
      "loss: 0.440126 [ 6464/60000]\n",
      "loss: 0.267647 [12864/60000]\n",
      "loss: 0.455117 [19264/60000]\n",
      "loss: 0.367543 [25664/60000]\n",
      "loss: 0.396409 [32064/60000]\n",
      "loss: 0.397785 [38464/60000]\n",
      "loss: 0.607486 [44864/60000]\n",
      "loss: 0.520467 [51264/60000]\n",
      "loss: 0.389712 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.446984 \n",
      "\n",
      "Epoch 85\n",
      "--------------\n",
      "loss: 0.243385 [   64/60000]\n",
      "loss: 0.438416 [ 6464/60000]\n",
      "loss: 0.266702 [12864/60000]\n",
      "loss: 0.454055 [19264/60000]\n",
      "loss: 0.364841 [25664/60000]\n",
      "loss: 0.395822 [32064/60000]\n",
      "loss: 0.396505 [38464/60000]\n",
      "loss: 0.605701 [44864/60000]\n",
      "loss: 0.518932 [51264/60000]\n",
      "loss: 0.388581 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.445873 \n",
      "\n",
      "Epoch 86\n",
      "--------------\n",
      "loss: 0.241680 [   64/60000]\n",
      "loss: 0.436747 [ 6464/60000]\n",
      "loss: 0.265731 [12864/60000]\n",
      "loss: 0.453051 [19264/60000]\n",
      "loss: 0.362126 [25664/60000]\n",
      "loss: 0.395180 [32064/60000]\n",
      "loss: 0.395326 [38464/60000]\n",
      "loss: 0.603888 [44864/60000]\n",
      "loss: 0.517309 [51264/60000]\n",
      "loss: 0.387518 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.444774 \n",
      "\n",
      "Epoch 87\n",
      "--------------\n",
      "loss: 0.240076 [   64/60000]\n",
      "loss: 0.435052 [ 6464/60000]\n",
      "loss: 0.264781 [12864/60000]\n",
      "loss: 0.452024 [19264/60000]\n",
      "loss: 0.359538 [25664/60000]\n",
      "loss: 0.394513 [32064/60000]\n",
      "loss: 0.394097 [38464/60000]\n",
      "loss: 0.601986 [44864/60000]\n",
      "loss: 0.515670 [51264/60000]\n",
      "loss: 0.386334 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.443690 \n",
      "\n",
      "Epoch 88\n",
      "--------------\n",
      "loss: 0.238498 [   64/60000]\n",
      "loss: 0.433257 [ 6464/60000]\n",
      "loss: 0.263815 [12864/60000]\n",
      "loss: 0.450969 [19264/60000]\n",
      "loss: 0.357098 [25664/60000]\n",
      "loss: 0.393982 [32064/60000]\n",
      "loss: 0.392976 [38464/60000]\n",
      "loss: 0.600164 [44864/60000]\n",
      "loss: 0.514050 [51264/60000]\n",
      "loss: 0.385232 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.442617 \n",
      "\n",
      "Epoch 89\n",
      "--------------\n",
      "loss: 0.237009 [   64/60000]\n",
      "loss: 0.431356 [ 6464/60000]\n",
      "loss: 0.262890 [12864/60000]\n",
      "loss: 0.449965 [19264/60000]\n",
      "loss: 0.354680 [25664/60000]\n",
      "loss: 0.393383 [32064/60000]\n",
      "loss: 0.391839 [38464/60000]\n",
      "loss: 0.598364 [44864/60000]\n",
      "loss: 0.512354 [51264/60000]\n",
      "loss: 0.384162 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.441567 \n",
      "\n",
      "Epoch 90\n",
      "--------------\n",
      "loss: 0.235584 [   64/60000]\n",
      "loss: 0.429546 [ 6464/60000]\n",
      "loss: 0.261930 [12864/60000]\n",
      "loss: 0.448886 [19264/60000]\n",
      "loss: 0.352276 [25664/60000]\n",
      "loss: 0.392773 [32064/60000]\n",
      "loss: 0.390757 [38464/60000]\n",
      "loss: 0.596647 [44864/60000]\n",
      "loss: 0.510713 [51264/60000]\n",
      "loss: 0.383062 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.440516 \n",
      "\n",
      "Epoch 91\n",
      "--------------\n",
      "loss: 0.234170 [   64/60000]\n",
      "loss: 0.427769 [ 6464/60000]\n",
      "loss: 0.261032 [12864/60000]\n",
      "loss: 0.447795 [19264/60000]\n",
      "loss: 0.349905 [25664/60000]\n",
      "loss: 0.392194 [32064/60000]\n",
      "loss: 0.389602 [38464/60000]\n",
      "loss: 0.594757 [44864/60000]\n",
      "loss: 0.509109 [51264/60000]\n",
      "loss: 0.382079 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.439481 \n",
      "\n",
      "Epoch 92\n",
      "--------------\n",
      "loss: 0.232834 [   64/60000]\n",
      "loss: 0.426021 [ 6464/60000]\n",
      "loss: 0.260077 [12864/60000]\n",
      "loss: 0.446743 [19264/60000]\n",
      "loss: 0.347581 [25664/60000]\n",
      "loss: 0.391613 [32064/60000]\n",
      "loss: 0.388517 [38464/60000]\n",
      "loss: 0.592860 [44864/60000]\n",
      "loss: 0.507624 [51264/60000]\n",
      "loss: 0.380970 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.438463 \n",
      "\n",
      "Epoch 93\n",
      "--------------\n",
      "loss: 0.231541 [   64/60000]\n",
      "loss: 0.424330 [ 6464/60000]\n",
      "loss: 0.259191 [12864/60000]\n",
      "loss: 0.445671 [19264/60000]\n",
      "loss: 0.345324 [25664/60000]\n",
      "loss: 0.391147 [32064/60000]\n",
      "loss: 0.387367 [38464/60000]\n",
      "loss: 0.591056 [44864/60000]\n",
      "loss: 0.506123 [51264/60000]\n",
      "loss: 0.379915 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.437457 \n",
      "\n",
      "Epoch 94\n",
      "--------------\n",
      "loss: 0.230281 [   64/60000]\n",
      "loss: 0.422641 [ 6464/60000]\n",
      "loss: 0.258387 [12864/60000]\n",
      "loss: 0.444664 [19264/60000]\n",
      "loss: 0.343178 [25664/60000]\n",
      "loss: 0.390653 [32064/60000]\n",
      "loss: 0.386382 [38464/60000]\n",
      "loss: 0.589381 [44864/60000]\n",
      "loss: 0.504635 [51264/60000]\n",
      "loss: 0.378915 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.436469 \n",
      "\n",
      "Epoch 95\n",
      "--------------\n",
      "loss: 0.229107 [   64/60000]\n",
      "loss: 0.420922 [ 6464/60000]\n",
      "loss: 0.257588 [12864/60000]\n",
      "loss: 0.443593 [19264/60000]\n",
      "loss: 0.341024 [25664/60000]\n",
      "loss: 0.390178 [32064/60000]\n",
      "loss: 0.385348 [38464/60000]\n",
      "loss: 0.587612 [44864/60000]\n",
      "loss: 0.503137 [51264/60000]\n",
      "loss: 0.377979 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.435492 \n",
      "\n",
      "Epoch 96\n",
      "--------------\n",
      "loss: 0.227933 [   64/60000]\n",
      "loss: 0.419186 [ 6464/60000]\n",
      "loss: 0.256787 [12864/60000]\n",
      "loss: 0.442638 [19264/60000]\n",
      "loss: 0.338971 [25664/60000]\n",
      "loss: 0.389716 [32064/60000]\n",
      "loss: 0.384319 [38464/60000]\n",
      "loss: 0.585682 [44864/60000]\n",
      "loss: 0.501751 [51264/60000]\n",
      "loss: 0.377057 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.434527 \n",
      "\n",
      "Epoch 97\n",
      "--------------\n",
      "loss: 0.226781 [   64/60000]\n",
      "loss: 0.417436 [ 6464/60000]\n",
      "loss: 0.255993 [12864/60000]\n",
      "loss: 0.441783 [19264/60000]\n",
      "loss: 0.336974 [25664/60000]\n",
      "loss: 0.389203 [32064/60000]\n",
      "loss: 0.383243 [38464/60000]\n",
      "loss: 0.583751 [44864/60000]\n",
      "loss: 0.500226 [51264/60000]\n",
      "loss: 0.376098 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.433554 \n",
      "\n",
      "Epoch 98\n",
      "--------------\n",
      "loss: 0.225675 [   64/60000]\n",
      "loss: 0.415760 [ 6464/60000]\n",
      "loss: 0.255300 [12864/60000]\n",
      "loss: 0.440872 [19264/60000]\n",
      "loss: 0.334999 [25664/60000]\n",
      "loss: 0.388568 [32064/60000]\n",
      "loss: 0.382240 [38464/60000]\n",
      "loss: 0.581964 [44864/60000]\n",
      "loss: 0.498634 [51264/60000]\n",
      "loss: 0.375126 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.432594 \n",
      "\n",
      "Epoch 99\n",
      "--------------\n",
      "loss: 0.224629 [   64/60000]\n",
      "loss: 0.414195 [ 6464/60000]\n",
      "loss: 0.254578 [12864/60000]\n",
      "loss: 0.439839 [19264/60000]\n",
      "loss: 0.333061 [25664/60000]\n",
      "loss: 0.388040 [32064/60000]\n",
      "loss: 0.381238 [38464/60000]\n",
      "loss: 0.580140 [44864/60000]\n",
      "loss: 0.497040 [51264/60000]\n",
      "loss: 0.374189 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.431617 \n",
      "\n",
      "Epoch 100\n",
      "--------------\n",
      "loss: 0.223631 [   64/60000]\n",
      "loss: 0.412502 [ 6464/60000]\n",
      "loss: 0.253906 [12864/60000]\n",
      "loss: 0.438815 [19264/60000]\n",
      "loss: 0.331184 [25664/60000]\n",
      "loss: 0.387508 [32064/60000]\n",
      "loss: 0.380263 [38464/60000]\n",
      "loss: 0.578556 [44864/60000]\n",
      "loss: 0.495618 [51264/60000]\n",
      "loss: 0.373286 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.430685 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------\")\n",
    "    train(model, train_dataloader, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a39fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images: 10000 / Right guesses: 8468 / Wrong guesses: 1532\n",
      "Accuracy: 84.68%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "pred_array = []\n",
    "acc_array = []\n",
    "\n",
    "model.eval()\n",
    "for i in range(len(test_data)):\n",
    "\n",
    "    x,y = test_data[i][0], test_data[i][1]\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device)\n",
    "        pred = model(x)\n",
    "        predicted, actual = classes[pred[0].argmax(0)], classes[y] # need to understand this line\n",
    "\n",
    "        pred_array.append(1)\n",
    "\n",
    "        if predicted == actual:\n",
    "            acc_array.append(1)\n",
    "\n",
    "        # print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
    "print(f\"Total Images: {sum(pred_array)} / Right guesses: {sum(acc_array)} / Wrong guesses: {sum(pred_array)-sum(acc_array)}\\nAccuracy: {sum(acc_array)/sum(pred_array)*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d5ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "# f1 score\n",
    "# auc curve etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
